{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4626c8-025d-4133-ba0c-1075adcdb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #1: What is Regularization and Its Importance\n",
    "\n",
    "\n",
    "#Regularization is a technique used in deep learning to prevent a model from fitting too closely to the training data. Imagine trying to memorize a book word-for-word; while you might ace a quiz on that book, you wouldn't be able to apply its concepts in real life. Similarly, a model that overfits the training data performs poorly on unseen data. Regularization introduces a penalty for complexity, encouraging the model to learn more general patterns rather than noise. This is vital because it helps ensure that the model can generalize well to new, unseen data, which is the ultimate goal of any machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f30fbbf-4f8b-4781-ba03-557f17ee16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #2: The Bias-Variance Tradeoff and Regularization\n",
    "#The bias-variance tradeoff is a fundamental concept in machine learning. Bias refers to the error due to overly simplistic assumptions in the learning algorithm, while variance refers to the error due to excessive sensitivity to fluctuations in the training data. Regularization helps balance this tradeoff by adding a penalty term to the loss function, which discourages the model from becoming too complex. By doing so, it reduces variance without significantly increasing bias, leading to a model that performs better on unseen data. Think of it as a coach guiding an athlete to focus on technique rather than just speed; the athlete becomes more well-rounded and effective in competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f30ebbd-bb4f-48f2-819a-c636da7c99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    " #3: L1 vs. L2 Regularization\n",
    "##L1 and L2 regularization are two popular techniques used to impose penalties on model complexity. L1 regularization, also known as Lasso, adds the absolute value of the coefficients as a penalty term to the loss function. This can lead to sparse models, where some weights become zero, effectively performing feature selection. On the other hand, L2 regularization, or Ridge, adds the square of the coefficients as a penalty. This tends to shrink the weights but usually keeps all features in the model. The key difference lies in how they penalize the weights: L1 can eliminate some weights entirely, while L2 shrinks them but retains all. Both methods help in reducing overfitting, but the choice between them can depend on the specific problem and dataset.\n",
    "\n",
    "##### In summary, regularization is a powerful tool in deep learning that helps create models that generalize better to new data, balancing the bias-variance tradeoff and offering different approaches through L1 and L2 techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa5000-130c-4318-951a-e1a39c7ce392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d2b7d5c-8f97-4ecd-a376-79bae90a0545",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m model_with_dropout\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m                             loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m                             metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model_with_dropout\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_images\u001b[49m, train_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model_with_dropout\u001b[38;5;241m.\u001b[39mevaluate(test_images, test_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "#Part 3: Applying Regularizatioo\n",
    "#Ák Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate\n",
    "#its impact on model performance and compare it with a model without Dropoutk\n",
    "# ́k Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a\n",
    "#given deep learning task.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Create a simple model\n",
    "model_with_dropout = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_with_dropout.compile(optimizer='adam',\n",
    "                            loss='sparse_categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_with_dropout.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model_with_dropout.evaluate(test_images, test_labels)\n",
    "print(f'Model with Dropout - Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "183b4e2e-6aff-47cf-b4f7-76db0dcc0ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m model_without_dropout\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                                loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m                                metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_without_dropout\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_images\u001b[49m, train_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss_no_dropout, accuracy_no_dropout \u001b[38;5;241m=\u001b[39m model_without_dropout\u001b[38;5;241m.\u001b[39mevaluate(test_images, test_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a model without Dropout\n",
    "model_without_dropout = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_without_dropout.compile(optimizer='adam',\n",
    "                               loss='sparse_categorical_crossentropy',\n",
    "                               metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_without_dropout.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "loss_no_dropout, accuracy_no_dropout = model_without_dropout.evaluate(test_images, test_labels)\n",
    "print(f'Model without Dropout - Loss: {loss_no_dropout}, Accuracy: {accuracy_no_dropout}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546e044-d544-45e6-9f33-6ebd01845c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
